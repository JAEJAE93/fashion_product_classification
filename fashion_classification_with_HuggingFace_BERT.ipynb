{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fashion_classification_with_HuggingFace_BERT",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7T2xvsQtl1yIzkdq42d2e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JAEJAE93/fashion_product_classification/blob/main/fashion_classification_with_HuggingFace_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opJaFOGW8_nJ"
      },
      "source": [
        "# Hugging Face의 트랜스포머 모델을 설치\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nunuPmU29ZOw",
        "outputId": "9d18371a-3bf2-4b49-84b9-9181a9799e40"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Mar 31 06:48:05 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcxTZDaD9kKQ",
        "outputId": "1cd0125b-b825-46d0-df89-4114269acbab"
      },
      "source": [
        "# Google drive 접근\n",
        "# http://blog.naver.com/PostView.nhn?blogId=skk1991&logNo=221746903289\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/')\n",
        "%cd gdrive/MyDrive/'Colab Notebooks'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "/content/gdrive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1jDEnBY9_72"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import datetime"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "Q6QhAqFV97PT",
        "outputId": "32432011-8ab6-41f7-809c-a58f564eef31"
      },
      "source": [
        "data_path = 'fashion_data/'\n",
        "pkl_path = data_path + 'fashion_data.pkl'\n",
        "\n",
        "# pkl load\n",
        "with open(pkl_path, \"rb\") as f:\n",
        "  total_data = pickle.load(f)\n",
        "\n",
        "print(\"total_data: \", total_data.shape)\n",
        "print(\"product category: \", (total_data[\"상품카테고리\"].unique()))\n",
        "total_data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total_data:  (31614, 3)\n",
            "product category:  ['지갑' '스웨터' '점퍼/아우터' '코트' '자켓' '팬츠' '신발' '티셔츠' '가방' '트렌치 코트' '모자' '드레스'\n",
            " '머플러/마스크' '홀더 및 케이스' '양말' '스카프' '블라우스' '셔츠' '넥타이' '베스트' '스커트' '장갑' '벨트'\n",
            " '주얼리 및 타이핀']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>상품설명</th>\n",
              "      <th>상품카테고리</th>\n",
              "      <th>상품컬러</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>블랙 가죽 골드 퍼피로고 카드지갑</td>\n",
              "      <td>지갑</td>\n",
              "      <td>BLACK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[인기상품 재입고] 오렌지 19FW 리버서블 풀오버 니트</td>\n",
              "      <td>스웨터</td>\n",
              "      <td>NORMAL ORANGE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[셀럽 협찬 주X욱 착용] 머스타드 《가먼트다잉》 실키 후드점퍼</td>\n",
              "      <td>점퍼/아우터</td>\n",
              "      <td>LIGHT YELLOW</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>그레이 짜임패턴 울캐시미어혼방 니트</td>\n",
              "      <td>스웨터</td>\n",
              "      <td>LIGHT GREY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[LE RIRE] 베이지 단색 울캐시미어혼방 니트</td>\n",
              "      <td>스웨터</td>\n",
              "      <td>NORMAL BEIGE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31609</th>\n",
              "      <td>[ILFORD]블랙 엠보 가죽 베이직 그립백</td>\n",
              "      <td>가방</td>\n",
              "      <td>BLACK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31610</th>\n",
              "      <td>[HUDSON]블랙 가죽 H금속 로고 클러치백(M)</td>\n",
              "      <td>가방</td>\n",
              "      <td>BLACK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31611</th>\n",
              "      <td>[LINDBERG] 블랙 화이트 로고 모던 클러치</td>\n",
              "      <td>지갑</td>\n",
              "      <td>BLACK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31612</th>\n",
              "      <td>블랙 위빙 가죽 장지갑</td>\n",
              "      <td>지갑</td>\n",
              "      <td>BLACK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31613</th>\n",
              "      <td>블랙 퍼피로고 여행용 장지갑</td>\n",
              "      <td>지갑</td>\n",
              "      <td>BLACK</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>31614 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      상품설명  상품카테고리           상품컬러\n",
              "0                       블랙 가죽 골드 퍼피로고 카드지갑      지갑          BLACK\n",
              "1          [인기상품 재입고] 오렌지 19FW 리버서블 풀오버 니트     스웨터  NORMAL ORANGE\n",
              "2      [셀럽 협찬 주X욱 착용] 머스타드 《가먼트다잉》 실키 후드점퍼  점퍼/아우터   LIGHT YELLOW\n",
              "3                      그레이 짜임패턴 울캐시미어혼방 니트     스웨터     LIGHT GREY\n",
              "4              [LE RIRE] 베이지 단색 울캐시미어혼방 니트     스웨터   NORMAL BEIGE\n",
              "...                                    ...     ...            ...\n",
              "31609             [ILFORD]블랙 엠보 가죽 베이직 그립백      가방          BLACK\n",
              "31610         [HUDSON]블랙 가죽 H금속 로고 클러치백(M)      가방          BLACK\n",
              "31611          [LINDBERG] 블랙 화이트 로고 모던 클러치      지갑          BLACK\n",
              "31612                         블랙 위빙 가죽 장지갑      지갑          BLACK\n",
              "31613                      블랙 퍼피로고 여행용 장지갑      지갑          BLACK\n",
              "\n",
              "[31614 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iM47gcR9Op2"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0U0NyxN763W"
      },
      "source": [
        "def _preprocessing(text):\n",
        "    \"\"\"\n",
        "        input: string\n",
        "        output: string\n",
        "    \"\"\"\n",
        "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'  # E-mail제거\n",
        "    text = re.sub(pattern = pattern, repl = '', string = text)\n",
        "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
        "    text = re.sub(pattern = pattern, repl = '', string = text)\n",
        "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'  # 한글 자음, 모음 제거\n",
        "    text = re.sub(pattern = pattern, repl = '', string = text)\n",
        "    pattern = '<[^>]*>'  # HTML 태그 제거\n",
        "    text = re.sub(pattern = pattern, repl = '', string = text)\n",
        "    pattern = '[^\\w\\s]'  # 특수기호제거\n",
        "    text = re.sub(pattern = pattern, repl = ' ', string = text)  \n",
        "    text = ' '.join(text.split()) # 2개 이상의 공백 제거\n",
        "    text = text.lower()\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "-fc1TYIC8SX1",
        "outputId": "e617be95-1bcd-4db8-a5bf-b9ef2a622a2c"
      },
      "source": [
        "text_data = total_data[[\"상품설명\", \"상품카테고리\"]]\n",
        "\n",
        "# label encoder\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "label_encoder.fit(text_data[\"상품카테고리\"])\n",
        "text_data[\"상품카테고리\"] = label_encoder.transform(text_data[\"상품카테고리\"])\n",
        "\n",
        "# text preprocessing\n",
        "text_data[\"상품설명\"] = [_preprocessing(x) for x in text_data[\"상품설명\"]]\n",
        "text_data = text_data.reset_index(drop=True)\n",
        "text_data"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>상품설명</th>\n",
              "      <th>상품카테고리</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>블랙 가죽 골드 퍼피로고 카드지갑</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>인기상품 재입고 오렌지 19fw 리버서블 풀오버 니트</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>셀럽 협찬 주x욱 착용 머스타드 가먼트다잉 실키 후드점퍼</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>그레이 짜임패턴 울캐시미어혼방 니트</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>le rire 베이지 단색 울캐시미어혼방 니트</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31609</th>\n",
              "      <td>ilford 블랙 엠보 가죽 베이직 그립백</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31610</th>\n",
              "      <td>hudson 블랙 가죽 h금속 로고 클러치백 m</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31611</th>\n",
              "      <td>lindberg 블랙 화이트 로고 모던 클러치</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31612</th>\n",
              "      <td>블랙 위빙 가죽 장지갑</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31613</th>\n",
              "      <td>블랙 퍼피로고 여행용 장지갑</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>31614 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  상품설명  상품카테고리\n",
              "0                   블랙 가죽 골드 퍼피로고 카드지갑      18\n",
              "1        인기상품 재입고 오렌지 19fw 리버서블 풀오버 니트       9\n",
              "2      셀럽 협찬 주x욱 착용 머스타드 가먼트다잉 실키 후드점퍼      16\n",
              "3                  그레이 짜임패턴 울캐시미어혼방 니트       9\n",
              "4            le rire 베이지 단색 울캐시미어혼방 니트       9\n",
              "...                                ...     ...\n",
              "31609          ilford 블랙 엠보 가죽 베이직 그립백       0\n",
              "31610       hudson 블랙 가죽 h금속 로고 클러치백 m       0\n",
              "31611        lindberg 블랙 화이트 로고 모던 클러치      18\n",
              "31612                     블랙 위빙 가죽 장지갑      18\n",
              "31613                  블랙 퍼피로고 여행용 장지갑      18\n",
              "\n",
              "[31614 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "Y1EnGvlr9Osx",
        "outputId": "30fa7b81-dd95-44a1-c9ab-a9bad28dca04"
      },
      "source": [
        "product_category = text_data[\"상품카테고리\"].unique()\n",
        "print(\"product_category num: \", len(product_category))\n",
        "print(\"product_category: \", product_category)\n",
        "\n",
        "train, test = train_test_split(text_data, test_size=0.25, random_state=42)\n",
        "print(\"train: \", train.shape)\n",
        "print(\"test: \", test.shape)\n",
        "train"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "product_category num:  24\n",
            "product_category:  [18  9 16 19 14 22 12 21  0 20  4  2  3 23 13 10  7  8  1  5 11 15  6 17]\n",
            "train:  (23710, 2)\n",
            "test:  (7904, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>상품설명</th>\n",
              "      <th>상품카테고리</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4610</th>\n",
              "      <td>그린 컬러배색 모피 퍼코트</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21025</th>\n",
              "      <td>mmlg knit bucket black</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10645</th>\n",
              "      <td>air line a1 블랙 4way 백팩</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22341</th>\n",
              "      <td>네이비 포켓 울 니트베스트</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30799</th>\n",
              "      <td>argyle fringe knit muffler_cream</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29802</th>\n",
              "      <td>lord nermal ls gold</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5390</th>\n",
              "      <td>블랙 가죽 로고 클러치백</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>그린 꼬르소 이지 스웻</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15795</th>\n",
              "      <td>stripe pattern 네이비 버티컬 스트라이프 울 홑겹 베스트</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23654</th>\n",
              "      <td>그레이 벨티드 울 롱코트</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23710 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        상품설명  상품카테고리\n",
              "4610                          그린 컬러배색 모피 퍼코트      16\n",
              "21025                 mmlg knit bucket black       4\n",
              "10645                 air line a1 블랙 4way 백팩       0\n",
              "22341                         네이비 포켓 울 니트베스트       9\n",
              "30799       argyle fringe knit muffler_cream       3\n",
              "...                                      ...     ...\n",
              "29802                    lord nermal ls gold      21\n",
              "5390                           블랙 가죽 로고 클러치백       0\n",
              "860                             그린 꼬르소 이지 스웻      21\n",
              "15795  stripe pattern 네이비 버티컬 스트라이프 울 홑겹 베스트       9\n",
              "23654                          그레이 벨티드 울 롱코트      19\n",
              "\n",
              "[23710 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYa2yPdO_3I_"
      },
      "source": [
        "# 전처리 - 훈련셋"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM_5vbrdMbnm"
      },
      "source": [
        "# BERT 입력 형식에 맞게 변환\n",
        "def _get_bert_sentences(sentences):\n",
        "    sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "# 라벨 추출\n",
        "def _get_label(targets, column):\n",
        "    labels = targets[column].values\n",
        "\n",
        "    return labels\n",
        "\n",
        "# BERT의 토크나이저로 문장을 토큰으로 분리\n",
        "def _get_tokenized_texts(version, sentences):\n",
        "    tokenizer = BertTokenizer.from_pretrained(version, do_lower_case=False)\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "    return tokenizer, tokenized_texts\n",
        "\n",
        "# 토큰화된 문장 숫자로 변경\n",
        "def _get_input_ids(MAX_LEN, tokenizer, tokenized_texts): # MAX_LEN = 입력 토큰의 최대 시퀀스 길이\n",
        "    # 토큰을 숫자 인덱스로 변환\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\n",
        "    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    return input_ids\n",
        "\n",
        "# attention mask 얻기\n",
        "def _get_attention_masks(input_ids):\n",
        "    # 어텐션 마스크 초기화\n",
        "    attention_masks = []\n",
        "\n",
        "    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i > 0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    return attention_masks\n",
        "\n",
        "# 데이터를 파이토치의 텐서로 변환\n",
        "def _get_torch_tensor(input_ids, labels, attention_masks):\n",
        "    test_inputs = torch.tensor(input_ids)\n",
        "    test_labels = torch.tensor(labels)\n",
        "    test_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return test_inputs, test_labels, test_masks"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brynGzny__gA"
      },
      "source": [
        "![대체 텍스트](https://mino-park7.github.io/images/2019/02/bert-input-representation.png)\n",
        "\n",
        "BERT의 입력은 위의 그림과 같은 형식입니다. Classification을 뜻하는 [CLS] 심볼이 제일 앞에 삽입됩니다. 파인튜닝시 출력에서 이 위치의 값을 사용하여 분류를 합니다. [SEP]은 Seperation을 가리키는데, 두 문장을 구분하는 역할을 합니다. 이 예제에서는 문장이 하나이므로 [SEP]도 하나만 넣습니다.\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uXx0o_w9Ovg",
        "outputId": "7deb59ec-c538-4eac-9ac7-0ca753f625f7"
      },
      "source": [
        "# BERT 입력 형식에 맞게 변환\n",
        "sentences = _get_bert_sentences(train[\"상품설명\"])\n",
        "sentences[:10]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] 그린 컬러배색 모피 퍼코트 [SEP]',\n",
              " '[CLS] mmlg knit bucket black [SEP]',\n",
              " '[CLS] air line a1 블랙 4way 백팩 [SEP]',\n",
              " '[CLS] 네이비 포켓 울 니트베스트 [SEP]',\n",
              " '[CLS] argyle fringe knit muffler_cream [SEP]',\n",
              " '[CLS] 블랙 차이나넥 언발란스 자켓 [SEP]',\n",
              " '[CLS] 카멜 폴리 혼방 부클 롱 자켓 [SEP]',\n",
              " '[CLS] 무지 숏 기모 맨투맨 블랙 엠오엠씨 [SEP]',\n",
              " '[CLS] 크림 부클내피 탈부착 면혼방 후드캐주얼점퍼 [SEP]',\n",
              " '[CLS] 화이트 라인배색 면 롱셔츠 [SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGmHWSMv9OyZ",
        "outputId": "b369b609-f13a-4ca2-c771-5c13e2aed534"
      },
      "source": [
        "# 라벨 추출\n",
        "labels = _get_label(train, '상품카테고리')\n",
        "labels"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([16,  4,  0, ..., 21,  9, 19])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW7ejVPn9O1U",
        "outputId": "f8323c50-ac9e-456f-b7fd-de0df1ba92bd"
      },
      "source": [
        "# BERT의 토크나이저로 문장을 토큰으로 분리\n",
        "\n",
        "tokenizer, tokenized_texts = _get_tokenized_texts('bert-base-multilingual-cased', sentences)\n",
        "\n",
        "print (sentences[0])\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] 그린 컬러배색 모피 퍼코트 [SEP]\n",
            "['[CLS]', '그', '##린', '컬', '##러', '##배', '##색', '모', '##피', '퍼', '##코', '##트', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljRQjmmp9O4J",
        "outputId": "a6879851-d077-43e7-f123-5822edf7a998"
      },
      "source": [
        "input_ids = _get_input_ids(128, tokenizer, tokenized_texts)\n",
        "input_ids[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  101,  8924, 27654,  9801, 30873, 76036, 41442,  9283, 97146,\n",
              "        9913, 25517, 15184,   102,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qzx3Jg_9O66",
        "outputId": "d48df08c-225c-4da2-b163-ef37fe004158"
      },
      "source": [
        "# 어텐션 마스크 얻기\n",
        "\n",
        "attention_masks = _get_attention_masks(input_ids)\n",
        "\n",
        "print(attention_masks[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSyiEUn_9O9y",
        "outputId": "b03356ef-ec87-47fd-c6ab-105031bbc529"
      },
      "source": [
        "# 훈련셋과 검증셋으로 분리\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
        "                                                                                    labels, \n",
        "                                                                                    random_state=42, \n",
        "                                                                                    test_size=0.1)\n",
        "\n",
        "# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
        "                                                       input_ids,\n",
        "                                                       random_state=42, \n",
        "                                                       test_size=0.1)\n",
        "\n",
        "# 데이터를 파이토치의 텐서로 변환\n",
        "# train\n",
        "train_inputs, train_labels, train_masks = _get_torch_tensor(train_inputs, train_labels, train_masks)\n",
        "# validation\n",
        "validation_inputs, validation_labels, validation_masks = _get_torch_tensor(validation_inputs, validation_labels, validation_masks)\n",
        "\n",
        "print(\"train_inputs: \", train_inputs.shape)\n",
        "print(\"train_labels: \", train_labels.shape)\n",
        "print(\"train_masks: \", train_masks.shape)\n",
        "print()\n",
        "print(\"validation_inputs: \", validation_inputs.shape)\n",
        "print(\"validation_labels: \", validation_labels.shape)\n",
        "print(\"validation_masks: \", validation_masks.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_inputs:  torch.Size([21339, 128])\n",
            "train_labels:  torch.Size([21339])\n",
            "train_masks:  torch.Size([21339, 128])\n",
            "\n",
            "validation_inputs:  torch.Size([2371, 128])\n",
            "validation_labels:  torch.Size([2371])\n",
            "validation_masks:  torch.Size([2371, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcz5qyZXSCst",
        "outputId": "e092b5f2-e558-418c-f930-2d5f35cc3745"
      },
      "source": [
        "train_inputs"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101,  8924, 56645,  ...,     0,     0,     0],\n",
              "        [  101,  9638, 83616,  ...,     0,     0,     0],\n",
              "        [  101,  9376, 17342,  ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [  101, 18922, 23661,  ...,     0,     0,     0],\n",
              "        [  101,  9376, 17342,  ...,     0,     0,     0],\n",
              "        [  101,  9591, 33323,  ...,     0,     0,     0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97afj9NO9PAz"
      },
      "source": [
        "# 배치 사이즈\n",
        "batch_size = 32\n",
        "\n",
        "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
        "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDiHwJ-J_uWh"
      },
      "source": [
        "# 전처리 - 테스트셋"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4536adt9PDh",
        "outputId": "b921ff13-8c1d-4a14-f679-1538835499f9"
      },
      "source": [
        "sentences = test['상품설명']\n",
        "print(\"sentences: \", len(sentences))\n",
        "sentences[:10]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentences:  7904\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26218                               라이트그레이 골드 리본장식 탑지퍼 반지갑\n",
              "25688                      rainbow ny pt 레드 히든밴딩 면혼방 여성 팬츠\n",
              "10175                          다크그레이 솔리드 기본핏 울 혼방 스트레이트 팬츠\n",
              "19866    on line limited summer run 블랙 남성 폴리 스판 스포츠 ice...\n",
              "18983                        hartford women 카키 패턴 면 셔츠형원피스\n",
              "19100                                     다크블루 면혼방 8부스키니팬츠\n",
              "6562                                  alysi 옐로우 패턴 울혼방 가디건\n",
              "8658                                      블랙 가죽 실버 로고 서류가방\n",
              "1255                               아울렛new 화이트 이면배색 가죽 스니커즈\n",
              "18833                                      옐로우 프린트 실크 랩스커트\n",
              "Name: 상품설명, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qXnF33J9PGM",
        "outputId": "59f4af9b-941d-4f0f-deb1-6a3052c65665"
      },
      "source": [
        "# BERT 입력 형식에 맞게 변환\n",
        "sentences = _get_bert_sentences(test[\"상품설명\"])\n",
        "\n",
        "# 라벨 추출\n",
        "labels = _get_label(test, '상품카테고리')\n",
        "\n",
        "# BERT의 토크나이저로 문장을 토큰으로 분리\n",
        "tokenizer, tokenized_texts = _get_tokenized_texts('bert-base-multilingual-cased', sentences)\n",
        "\n",
        "# 토큰화된 문장 숫자로 변경\n",
        "input_ids = _get_input_ids(128, tokenizer, tokenized_texts)\n",
        "\n",
        "# 어텐션 마스크 얻기\n",
        "attention_masks = _get_attention_masks(input_ids)\n",
        "\n",
        "# 데이터를 파이토치의 텐서로 변환\n",
        "test_inputs, test_labels, test_masks = _get_torch_tensor(input_ids, labels, attention_masks)\n",
        "\n",
        "print(\"test_inputs: \", test_inputs.shape)\n",
        "print(\"test_labels: \", test_labels.shape)\n",
        "print(\"test_masks: \", test_masks.shape)\n",
        "\n",
        "# 배치 사이즈\n",
        "batch_size = 32\n",
        "\n",
        "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
        "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_inputs:  torch.Size([7904, 128])\n",
            "test_labels:  torch.Size([7904])\n",
            "test_masks:  torch.Size([7904, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAgDUe_UBOfo"
      },
      "source": [
        "# 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8lTHN6OAM86",
        "outputId": "843a4193-186b-4c83-dc9b-287c7ceb0bb8"
      },
      "source": [
        "# GPU 디바이스 이름 구함\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# GPU 디바이스 이름 검사\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkaZOdZ-AM_h",
        "outputId": "01a25647-a3ed-492b-da96-011839cb46fc"
      },
      "source": [
        "# 디바이스 설정\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available, using the CPU instead.')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye9DSX0SANCQ",
        "outputId": "5d28af35-0d11-4a76-ba95-f21622023c32"
      },
      "source": [
        "# 분류를 위한 BERT 모델 생성\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(product_category))\n",
        "model.cuda()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=24, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MSG-AARBlMX"
      },
      "source": [
        "![대체 텍스트](http://mccormickml.com/assets/BERT/padding_and_mask.png)\n",
        "\n",
        "사전훈련된 BERT는 다양한 문제로 전이학습이 가능합니다. 여기서는 위의 그림과 같이 한 문장을 분류하는 방법을 사용합니다. 영화리뷰 문장이 입력으로 들어가면, 긍정/부정으로 구분합니다. 모델의 출력에서 [CLS] 위치인 첫 번째 토큰에 새로운 레이어를 붙여서 파인튜닝을 합니다. Huggning Face는 BertForSequenceClassification() 함수를 제공하기 때문에 쉽게 구현할 수 있습니다.\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ml2jIqjANHr"
      },
      "source": [
        "# 옵티마이저 설정\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # 학습률\n",
        "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
        "                )\n",
        "\n",
        "# 에폭수\n",
        "epochs = 4\n",
        "\n",
        "# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# 처음에 학습률을 조금씩 변화시키는 스케줄러 생성\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=total_steps)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqW9cg8qB1mp"
      },
      "source": [
        "# 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GUeT7JJByra"
      },
      "source": [
        "# 정확도 계산 함수\n",
        "def flat_accuracy(preds, labels):\n",
        "    \n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# 시간 표시 함수\n",
        "def format_time(elapsed):\n",
        "\n",
        "    # 반올림\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # hh:mm:ss으로 형태 변경\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goxZk6wyClhk"
      },
      "source": [
        "# colab 튕김 방지\n",
        "\n",
        "function ClickConnect() {var buttons = document.querySelectorAll(\"colab-dialog.yes-no-dialog paper-button#cancel\"); buttons.forEach(function(btn) { btn.click(); }); console.log(\"1분마다 자동 재연결\"); document.querySelector(\"colab-toolbar-button#connect\").click(); } setInterval(ClickConnect,1000*60);\n",
        "\n",
        "https://somjang.tistory.com/entry/Google-Colab-%EC%9E%90%EC%A3%BC%EB%81%8A%EA%B8%B0%EB%8A%94-%EB%9F%B0%ED%83%80%EC%9E%84-%EB%B0%A9%EC%A7%80%ED%95%98%EA%B8%B0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tmrAi82ByuR",
        "outputId": "84c77328-81e4-4e99-b941-8f8f866e8c5e"
      },
      "source": [
        "# 재현을 위해 랜덤시드 고정\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# 그래디언트 초기화\n",
        "model.zero_grad()\n",
        "\n",
        "# 에폭만큼 반복\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # 시작 시간 설정\n",
        "    t0 = time.time()\n",
        "\n",
        "    # 로스 초기화\n",
        "    total_loss = 0\n",
        "\n",
        "    # 훈련모드로 변경\n",
        "    model.train()\n",
        "        \n",
        "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # 경과 정보 표시\n",
        "        if step % 500 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # 배치를 GPU에 넣음\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # 배치에서 데이터 추출\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Forward 수행                \n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "        \n",
        "        # 로스 구함\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # 총 로스 계산\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward 수행으로 그래디언트 계산\n",
        "        loss.backward()\n",
        "\n",
        "        # 그래디언트 클리핑\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 스케줄러로 학습률 감소\n",
        "        scheduler.step()\n",
        "\n",
        "        # 그래디언트 초기화\n",
        "        model.zero_grad()\n",
        "\n",
        "    # 평균 로스 계산\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    #시작 시간 설정\n",
        "    t0 = time.time()\n",
        "\n",
        "    # 평가모드로 변경\n",
        "    model.eval()\n",
        "\n",
        "    # 변수 초기화\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "    for batch in validation_dataloader:\n",
        "        # 배치를 GPU에 넣음\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # 배치에서 데이터 추출\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # 그래디언트 계산 안함\n",
        "        with torch.no_grad():     \n",
        "            # Forward 수행\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # 로스 구함\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # CPU로 데이터 이동\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    print(\"  Accuracy: {0:.4f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch   500  of    667.    Elapsed: 0:05:49.\n",
            "\n",
            "  Average training loss: 0.83\n",
            "  Training epcoh took: 0:07:45\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.9231\n",
            "  Validation took: 0:00:18\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch   500  of    667.    Elapsed: 0:05:48.\n",
            "\n",
            "  Average training loss: 0.27\n",
            "  Training epcoh took: 0:07:44\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.9372\n",
            "  Validation took: 0:00:18\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch   500  of    667.    Elapsed: 0:05:48.\n",
            "\n",
            "  Average training loss: 0.19\n",
            "  Training epcoh took: 0:07:45\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.9418\n",
            "  Validation took: 0:00:18\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch   500  of    667.    Elapsed: 0:05:48.\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epcoh took: 0:07:44\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.9489\n",
            "  Validation took: 0:00:18\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWzS9-MDiH4l"
      },
      "source": [
        "# 모델 저장\n",
        "# https://tutorials.pytorch.kr/beginner/saving_loading_models.html\n",
        "\n",
        "model_PATH = \"classification_with_HuggingFace_BERT/\"\n",
        "\n",
        "torch.save(model.state_dict(), model_PATH + \"model_state_dict2.pt\")\n",
        "torch.save(optimizer.state_dict(), model_PATH + \"optimizer_state_dict2.pt\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51tBQVJaB_3L"
      },
      "source": [
        "# 테스트셋 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_hASZy9kUPn"
      },
      "source": [
        "def eval_model(device, model, test_dataloader):\n",
        "    #시작 시간 설정\n",
        "    t0 = time.time()\n",
        "\n",
        "    # 평가모드로 변경\n",
        "    model.eval()\n",
        "\n",
        "    # 변수 초기화\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "    for step, batch in enumerate(test_dataloader):\n",
        "        # 경과 정보 표시\n",
        "        if step % 100 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
        "\n",
        "        # 배치를 GPU에 넣음\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # 배치에서 데이터 추출\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # 그래디언트 계산 안함\n",
        "        with torch.no_grad():     \n",
        "            # Forward 수행\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # 로스 구함\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # CPU로 데이터 이동\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Accuracy: {0:.4f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"Test took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bperClKMsVk",
        "outputId": "9e41aa7a-17a4-429c-f18a-f848bb350181"
      },
      "source": [
        "# test set 평가\n",
        "\n",
        "eval_model(device, model, test_dataloader)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Batch   100  of    247.    Elapsed: 0:00:26.\n",
            "  Batch   200  of    247.    Elapsed: 0:00:50.\n",
            "\n",
            "Accuracy: 0.9538\n",
            "Test took: 0:01:01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPGI1ioYk7-f",
        "outputId": "feb38cfe-2ad1-4c2d-8273-124106887d05"
      },
      "source": [
        "# load model, optimizer 설정\n",
        "\n",
        "model_PATH = \"classification_with_HuggingFace_BERT/\"\n",
        "load_model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(product_category))\n",
        "load_model.cuda()\n",
        "\n",
        "# 옵티마이저 설정\n",
        "load_optimizer = AdamW(load_model.parameters(),\n",
        "                       lr = 2e-5, # 학습률,\n",
        "                       eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
        "                       )\n",
        "\n",
        "load_model.load_state_dict(torch.load(model_PATH + \"model_state_dict2.pt\"))\n",
        "load_optimizer.load_state_dict(torch.load(model_PATH + \"optimizer_state_dict2.pt\"))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hkVqtP1OOlJ",
        "outputId": "3422f327-8ec2-41f7-894d-851fe77c4244"
      },
      "source": [
        "# load model 평가\n",
        "\n",
        "eval_model(device, load_model, test_dataloader)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Batch   100  of    247.    Elapsed: 0:00:25.\n",
            "  Batch   200  of    247.    Elapsed: 0:00:50.\n",
            "\n",
            "Accuracy: 0.9538\n",
            "Test took: 0:01:01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9wt5jQtCL8S"
      },
      "source": [
        "# 새로운 문장 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E2_QkkOByzk"
      },
      "source": [
        "# 입력 데이터 변환\n",
        "def convert_input_data(sentences):\n",
        "    # list로 들어옴.\n",
        "    sentences = ''.join(sentences) # list -> str\n",
        "    sentences = _preprocessing(sentences) # text 전처리\n",
        "    sentences = [sentences] # str -> array (list로 바꿔주는게 아니고 []를 사용해서 array로?..)\n",
        "\n",
        "    # BERT의 토크나이저로 문장을 토큰으로 분리\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "    print(\"tokenized_texts: \", tokenized_texts)\n",
        "\n",
        "    # 입력 토큰의 최대 시퀀스 길이\n",
        "    MAX_LEN = 128\n",
        "\n",
        "    # 토큰을 숫자 인덱스로 변환\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    \n",
        "    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    # 어텐션 마스크 초기화\n",
        "    attention_masks = []\n",
        "\n",
        "    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # 데이터를 파이토치의 텐서로 변환\n",
        "    inputs = torch.tensor(input_ids)\n",
        "    masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return inputs, masks\n",
        "\n",
        "# 문장 테스트\n",
        "def test_sentences(sentences):\n",
        "\n",
        "    # 평가모드로 변경\n",
        "    model.eval()\n",
        "\n",
        "    # 문장을 입력 데이터로 변환\n",
        "    inputs, masks = convert_input_data(sentences)\n",
        "\n",
        "    # 데이터를 GPU에 넣음\n",
        "    b_input_ids = inputs.to(device)\n",
        "    b_input_mask = masks.to(device)\n",
        "            \n",
        "    # 그래디언트 계산 안함\n",
        "    with torch.no_grad():     \n",
        "        # Forward 수행\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    # 로스 구함\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # CPU로 데이터 이동\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    return logits"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6sXKBjKbHpI",
        "outputId": "e2e0845b-b74b-49b0-b7cb-36b21f02a525"
      },
      "source": [
        "print(\"product category: \", (total_data[\"상품카테고리\"].unique()))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "product category:  ['지갑' '스웨터' '점퍼/아우터' '코트' '자켓' '팬츠' '신발' '티셔츠' '가방' '트렌치 코트' '모자' '드레스'\n",
            " '머플러/마스크' '홀더 및 케이스' '양말' '스카프' '블라우스' '셔츠' '넥타이' '베스트' '스커트' '장갑' '벨트'\n",
            " '주얼리 및 타이핀']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9rRGcZoYRVC"
      },
      "source": [
        "def predict_product_category(sent):\n",
        "  logits = test_sentences([sent])\n",
        "  predict_class = np.argmax(logits)\n",
        "  print(\"sent: {0}, predict: {1}\".format(sent, label_encoder.inverse_transform([predict_class])))\n",
        "\n",
        "  return logits"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxn3YFsHp-g-",
        "outputId": "60dcfe23-3eca-44f0-c3ab-cc5b08f7607e"
      },
      "source": [
        "# test\n",
        "\n",
        "predict_product_category('크록스 클린 산타 크루즈 컷 로퍼 남성용 202972-4BM')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenized_texts:  [['크', '##록', '##스', '클', '##린', '산', '##타', '크', '##루', '##즈', '컷', '로', '##퍼', '남', '##성', '##용', '202', '##97', '##2', '4', '##b', '##m']]\n",
            "sent: 크록스 클린 산타 크루즈 컷 로퍼 남성용 202972-4BM, predict: ['신발']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.32704595, -0.3970214 , -0.6286787 , -0.8869607 , -0.08011185,\n",
              "         0.6154584 , -0.73269767, -0.90003073,  0.25708804, -1.4971191 ,\n",
              "        -0.72188497, -0.8514829 ,  8.437163  , -0.13377562, -0.4716112 ,\n",
              "        -0.42228216, -0.14737742, -0.9253256 , -0.40350467, -0.42210567,\n",
              "        -0.8383144 ,  0.0645486 , -0.102325  , -0.6319899 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny-dv7DgYkLQ",
        "outputId": "b1929497-b896-4b2f-a505-b0a439340d6d"
      },
      "source": [
        "# Naver 쇼핑몰 데이터 분석\n",
        "\n",
        "test_list = [\n",
        "    '크록스 클린 산타 크루즈 컷 로퍼 남성용 202972-4BM', # 신발\n",
        "    '뉴발란스 남녀공용 런닝화 MR530SH', # 신발\n",
        "    '지오다노 남 테이퍼드 슬랙스 110511', # 바지\n",
        "    '지오다노 남 MW 테이퍼드 링클프리 110901', # 바지\n",
        "    '[아디다스]바시티 봄버 야구점퍼/GE1340', # 점퍼/아우터\n",
        "    '[미니멀프로젝트] 미니멀프로젝트 피그먼트 시그니쳐 오버핏 후드집업 MZT106 / 4color w', # 점퍼/아우터\n",
        "    '나이키 빅스우시 윈드 브레이커 바람막이 검흰 AR3132-010', # 점퍼/아우터\n",
        "    '[다이애그널] [29EDITION]_Pretzel puff knit - ivory', # 스웨터\n",
        "    '폴로 여성 울니트 Wool Vneck Sweater', # 스웨터\n",
        "    '나인식스뉴욕 식스뉴욕 96ny 코튼 테리 루즈핏 풀오버 4종', # 스웨터\n",
        "    '스튜디오톰보이 싱글 소매 리본 트랜치코트 9190111029 013', # 코트\n",
        "    '로엠 변형 홑겹 트렌치코트 RMJTA2301A', # 코트\n",
        "    '헨리코튼 코튼 린넨 혼방 피그먼트 다잉코트', # 코트\n",
        "    '칼하트 킥플립 백팩 블랙', # 가방\n",
        "    '루알리아 미니 피크닉 토트백', # 가방\n",
        "    '라코스테 레이디 미니 크로스백 NF2609' # 가방\n",
        "]\n",
        "\n",
        "product_logits_list = []\n",
        "\n",
        "for test_sent in test_list:\n",
        "  product_logits = predict_product_category(test_sent)\n",
        "  product_logits_list.append(product_logits.flatten()) # vector값 저장\n",
        "  print()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenized_texts:  [['크', '##록', '##스', '클', '##린', '산', '##타', '크', '##루', '##즈', '컷', '로', '##퍼', '남', '##성', '##용', '202', '##97', '##2', '4', '##b', '##m']]\n",
            "sent: 크록스 클린 산타 크루즈 컷 로퍼 남성용 202972-4BM, predict: ['신발']\n",
            "\n",
            "tokenized_texts:  [['뉴', '##발', '##란', '##스', '남', '##녀', '##공', '##용', '런', '##닝', '##화', 'm', '##r', '##53', '##0', '##sh']]\n",
            "sent: 뉴발란스 남녀공용 런닝화 MR530SH, predict: ['신발']\n",
            "\n",
            "tokenized_texts:  [['지', '##오', '##다', '##노', '남', '테', '##이', '##퍼', '##드', '슬', '##랙', '##스', '110', '##51', '##1']]\n",
            "sent: 지오다노 남 테이퍼드 슬랙스 110511, predict: ['팬츠']\n",
            "\n",
            "tokenized_texts:  [['지', '##오', '##다', '##노', '남', 'm', '##w', '테', '##이', '##퍼', '##드', '링', '##클', '##프', '##리', '110', '##90', '##1']]\n",
            "sent: 지오다노 남 MW 테이퍼드 링클프리 110901, predict: ['신발']\n",
            "\n",
            "tokenized_texts:  [['아', '##디', '##다', '##스', '바', '##시', '##티', '봄', '##버', '야구', '##점', '##퍼', 'ge', '##13', '##40']]\n",
            "sent: [아디다스]바시티 봄버 야구점퍼/GE1340, predict: ['점퍼/아우터']\n",
            "\n",
            "tokenized_texts:  [['미', '##니', '##멀', '##프', '##로', '##젝', '##트', '미', '##니', '##멀', '##프', '##로', '##젝', '##트', '피', '##그', '##먼', '##트', '시', '##그', '##니', '##쳐', '[UNK]', '후', '##드', '##집', '##업', 'm', '##zt', '##10', '##6', '4', '##color', 'w']]\n",
            "sent: [미니멀프로젝트] 미니멀프로젝트 피그먼트 시그니쳐 오버핏 후드집업 MZT106 / 4color w, predict: ['티셔츠']\n",
            "\n",
            "tokenized_texts:  [['나', '##이', '##키', '빅', '##스', '##우', '##시', '윈', '##드', '브', '##레', '##이', '##커', '바', '##람', '##막', '##이', '검', '##흰', 'ar', '##31', '##32', '010']]\n",
            "sent: 나이키 빅스우시 윈드 브레이커 바람막이 검흰 AR3132-010, predict: ['신발']\n",
            "\n",
            "tokenized_texts:  [['다', '##이', '##애', '##그', '##널', '29', '##edit', '##ion', '_', 'pret', '##zel', 'pu', '##ff', 'kn', '##it', 'i', '##vor', '##y']]\n",
            "sent: [다이애그널] [29EDITION]_Pretzel puff knit - ivory, predict: ['스웨터']\n",
            "\n",
            "tokenized_texts:  [['폴', '##로', '여성', '울', '##니', '##트', 'wo', '##ol', 'v', '##nec', '##k', 's', '##we', '##ater']]\n",
            "sent: 폴로 여성 울니트 Wool Vneck Sweater, predict: ['스웨터']\n",
            "\n",
            "tokenized_texts:  [['나', '##인', '##식', '##스', '##뉴', '##욕', '식', '##스', '##뉴', '##욕', '96', '##ny', '코', '##튼', '테', '##리', '[UNK]', '풀', '##오', '##버', '4', '##종']]\n",
            "sent: 나인식스뉴욕 식스뉴욕 96ny 코튼 테리 루즈핏 풀오버 4종, predict: ['양말']\n",
            "\n",
            "tokenized_texts:  [['스', '##튜', '##디오', '##톰', '##보', '##이', '싱글', '소', '##매', '리', '##본', '트', '##랜', '##치', '##코', '##트', '919', '##01', '##11', '##02', '##9', '013']]\n",
            "sent: 스튜디오톰보이 싱글 소매 리본 트랜치코트 9190111029 013, predict: ['코트']\n",
            "\n",
            "tokenized_texts:  [['로', '##엠', '변', '##형', '[UNK]', '트', '##렌', '##치', '##코', '##트', 'r', '##m', '##jta', '##23', '##01', '##a']]\n",
            "sent: 로엠 변형 홑겹 트렌치코트 RMJTA2301A, predict: ['코트']\n",
            "\n",
            "tokenized_texts:  [['헨', '##리', '##코', '##튼', '코', '##튼', '린', '##넨', '혼', '##방', '피', '##그', '##먼', '##트', '다', '##잉', '##코', '##트']]\n",
            "sent: 헨리코튼 코튼 린넨 혼방 피그먼트 다잉코트, predict: ['코트']\n",
            "\n",
            "tokenized_texts:  [['칼', '##하', '##트', '킥', '##플', '##립', '백', '##팩', '블', '##랙']]\n",
            "sent: 칼하트 킥플립 백팩 블랙, predict: ['가방']\n",
            "\n",
            "tokenized_texts:  [['루', '##알', '##리아', '미', '##니', '피', '##크', '##닉', '토', '##트', '##백']]\n",
            "sent: 루알리아 미니 피크닉 토트백, predict: ['가방']\n",
            "\n",
            "tokenized_texts:  [['라', '##코', '##스', '##테', '레', '##이', '##디', '미', '##니', '크', '##로', '##스', '##백', 'n', '##f', '##26', '##0', '##9']]\n",
            "sent: 라코스테 레이디 미니 크로스백 NF2609, predict: ['가방']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCTJ6OCoXGFe",
        "outputId": "beecf846-5839-47d2-bf82-6e3c37d68ced"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "corpus_embeddings = product_logits_list\n",
        "\n",
        "num_clusters = 6\n",
        "clustering_model = KMeans(n_clusters=num_clusters)\n",
        "clustering_model.fit(corpus_embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "clustered_sentences = [[] for i in range(num_clusters)]\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    clustered_sentences[cluster_id].append(test_list[sentence_id])\n",
        "\n",
        "for i, cluster in enumerate(clustered_sentences):\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster  1\n",
            "['크록스 클린 산타 크루즈 컷 로퍼 남성용 202972-4BM', '뉴발란스 남녀공용 런닝화 MR530SH', '지오다노 남 MW 테이퍼드 링클프리 110901', '나이키 빅스우시 윈드 브레이커 바람막이 검흰 AR3132-010']\n",
            "\n",
            "Cluster  2\n",
            "['스튜디오톰보이 싱글 소매 리본 트랜치코트 9190111029 013', '로엠 변형 홑겹 트렌치코트 RMJTA2301A', '헨리코튼 코튼 린넨 혼방 피그먼트 다잉코트']\n",
            "\n",
            "Cluster  3\n",
            "['칼하트 킥플립 백팩 블랙', '루알리아 미니 피크닉 토트백', '라코스테 레이디 미니 크로스백 NF2609']\n",
            "\n",
            "Cluster  4\n",
            "['[미니멀프로젝트] 미니멀프로젝트 피그먼트 시그니쳐 오버핏 후드집업 MZT106 / 4color w', '[다이애그널] [29EDITION]_Pretzel puff knit - ivory', '폴로 여성 울니트 Wool Vneck Sweater']\n",
            "\n",
            "Cluster  5\n",
            "['지오다노 남 테이퍼드 슬랙스 110511']\n",
            "\n",
            "Cluster  6\n",
            "['[아디다스]바시티 봄버 야구점퍼/GE1340', '나인식스뉴욕 식스뉴욕 96ny 코튼 테리 루즈핏 풀오버 4종']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E575PSCzp0r-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naqOkoHnp0uu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq8M9-ZNp0yA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}